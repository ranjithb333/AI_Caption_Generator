# -*- coding: utf-8 -*-
"""Image_Caption_Generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-OJjzo3t22zLyJk7WZ6y4jBBaiV_Uew3
"""

!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip
!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip
!unzip Flickr8k_Dataset.zip
!unzip Flickr8k_text.zip

from collections import defaultdict
import string

# Load the full captions file
def load_doc(filename):
    with open(filename, 'r') as file:
        return file.read()

# Parse only 1000 images
def load_descriptions_limited(doc, limit=1000):
    mapping = defaultdict(list)
    seen = set()
    for line in doc.split('\n'):
        tokens = line.strip().split('\t')
        if len(tokens) != 2: continue
        image_id, caption = tokens[0].split('#')[0], tokens[1]
        if image_id in seen or len(seen) < limit:
            seen.add(image_id)
            caption = caption.lower().translate(str.maketrans('', '', string.punctuation))
            mapping[image_id].append(f'startseq {caption} endseq')
        if len(seen) >= limit:
            break
    return dict(mapping)

doc = load_doc('Flickr8k.token.txt')
descriptions = load_descriptions_limited(doc, limit=1000)
print("Total images loaded:", len(descriptions))

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

import os
from keras.applications.inception_v3 import InceptionV3, preprocess_input
from keras.preprocessing import image
from keras.models import Model
import numpy as np

# InceptionV3 model
model = InceptionV3(weights='imagenet')
model_new = Model(model.input, model.layers[-2].output)

def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(299, 299))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    return preprocess_input(x)

# Extract features
features = {}
images_path = 'Flicker8k_Dataset'
for img_id in descriptions.keys():
    img_path = os.path.join(images_path, img_id)
    img = preprocess_image(img_path)
    feature = model_new.predict(img, verbose=0)
    features[img_id] = feature

print("Extracted features for", len(features), "images")

from tensorflow.keras.preprocessing.text import Tokenizer


all_captions = []
for cap_list in descriptions.values():
    all_captions.extend(cap_list)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_captions)
vocab_size = len(tokenizer.word_index) + 1
max_length = max(len(c.split()) for c in all_captions)
print("Vocabulary Size:", vocab_size)
print("Max Caption Length:", max_length)

from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import numpy as np

def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):
    X1, X2, y = [], [], []
    for key, desc_list in descriptions.items():
        for desc in desc_list:
            seq = tokenizer.texts_to_sequences([desc])[0]
            for i in range(1, len(seq)):
                in_seq, out_seq = seq[:i], seq[i]
                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
                out_seq = to_categorical(out_seq, num_classes=vocab_size)
                X1.append(photos[key].reshape(2048))  # Flatten
                X2.append(in_seq)
                y.append(out_seq)
    return np.array(X1), np.array(X2), np.array(y)


X1, X2, y = create_sequences(tokenizer, max_length, descriptions, features, vocab_size)

from keras.layers import Input, Dense, LSTM, Embedding, Dropout, add
from keras.models import Model
import pickle

# Define model architecture
inputs1 = Input(shape=(2048,))
fe1 = Dropout(0.5)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)

inputs2 = Input(shape=(max_length,))
se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
se2 = Dropout(0.5)(se1)
se3 = LSTM(256, use_cudnn=False)(se2) # Disable cuDNN

decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)

model = Model(inputs=[inputs1, inputs2], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam')

# Train
model.fit([X1, X2], y, epochs=10, batch_size=64)

# Save the model and tokenizer
model.save('caption_model.keras') # Save in native Keras format
with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

def word_for_id(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

def generate_caption(model, tokenizer, photo, max_length):
    in_text = 'startseq'
    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        yhat = model.predict([photo, sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = word_for_id(yhat, tokenizer)
        if word is None: break
        in_text += ' ' + word
        if word == 'endseq': break
    return in_text

# Load one image feature
photo = features['1000268201_693b08cb0e.jpg']
caption = generate_caption(model, tokenizer, photo, max_length)
print("Generated Caption:", caption)

import numpy as np
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Model

# Load InceptionV3 model (same as used for training)
def load_feature_extractor():
    model = InceptionV3(weights='imagenet')
    model_new = Model(model.input, model.layers[-2].output)  # Remove softmax
    return model_new

# Preprocess image (resize, convert to array, normalize)
def preprocess_image(img_path):
    img = load_img(img_path, target_size=(299, 299))
    x = img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    return x

# Extract features from image using InceptionV3
def extract_features(img_path, feature_model):
    img = preprocess_image(img_path)
    features = feature_model.predict(img, verbose=0)
    return features.reshape(1, 2048)

# Caption generation (same function as earlier)
def generate_caption(model, tokenizer, photo, max_length):
    in_text = 'startseq'
    for _ in range(max_length):
        seq = tokenizer.texts_to_sequences([in_text])[0]
        seq = pad_sequences([seq], maxlen=max_length)
        yhat = model.predict([photo, seq], verbose=0)
        yhat = np.argmax(yhat)
        word = word_for_id(yhat, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'endseq':
            break
    return in_text.replace('startseq', '').replace('endseq', '').strip()

# Convert token to word
def word_for_id(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

# Load your trained captioning model and tokenizer
from tensorflow.keras.models import load_model
import pickle

model = load_model('caption_model.keras') # Load from native Keras format

with open('tokenizer.pkl', 'rb') as f:
    tokenizer = pickle.load(f)

# Set max_length (same as training)
# max_length is already available from the previous steps
# max_length = 34  # Update this with your training value

# Path to external image
img_path = 'dog (1).jpg'  # Use the uploaded image

# Extract feature and generate caption
feature_model = load_feature_extractor()
photo = extract_features(img_path, feature_model)
caption = generate_caption(model, tokenizer, photo, max_length)

# Show result
print("Generated Caption:", caption)

# Show image
import matplotlib.pyplot as plt
from PIL import Image

plt.imshow(Image.open(img_path))
plt.axis('off')
plt.title(caption)
plt.show()

# Upload image
from google.colab import files
uploaded = files.upload()
img_path = list(uploaded.keys())[0]

# Extract feature
feature_model = load_feature_extractor()
photo = extract_features(img_path, feature_model)

# Generate caption
caption = generate_caption(model, tokenizer, photo, max_length)
print("Generated Caption:", caption)

# Show image
from PIL import Image
import matplotlib.pyplot as plt

plt.imshow(Image.open(img_path))
plt.title(caption)
plt.axis('off')
plt.show()

pip install tensorflow

